# -*- coding: utf-8 -*-
"""video_RETALKING_quick_demo_WORKING_25092025.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10lwOVtDPxrJtfgHMbvjSBd0itEP0WlZi

## VideoReTalkingï¼šAudio-based Lip Synchronization for Talking Head Video Editing In the Wild

[Arxiv](https://arxiv.org/abs/2211.14758) | [Project](https://vinthony.github.io/video-retalking/) | [Github](https://github.com/vinthony/video-retalking)

Kun Cheng, Xiaodong Cun, Yong Zhang, Menghan Xia, Fei Yin, Mingrui Zhu, Xuan Wang, Jue Wang, Nannan Wang

Xidian University, Tencent AI Lab, Tsinghua University

*SIGGRAPH Asia 2022 Conferenence Track*

# Verify GPU access
"""

!nvidia-smi

"""# Downgrade Python Version

[Explain](https://python.plainenglish.io/how-i-downgraded-python-in-google-colab-f4352d3288e3)

Thanks Jangachary Sriramadas
"""

!python --version

!sudo apt-get install python3.10

!sudo update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.10 1

!sudo update-alternatives --config python3

!python --version

!apt-get update
!apt install ffmpeg &> /dev/null

!curl -sS https://bootstrap.pypa.io/get-pip.py -o get-pip.py

!python3.10 get-pip.py

!python3.10 -m pip --version

!pip list

"""# Install Video-Retalking"""

!git clone https://github.com/Hemilibeatriz/video-retalking.git &> /dev/null

# Commented out IPython magic to ensure Python compatibility.
# %cd video-retalking

!pip install -r requirements.txt

"""**Download Pretrained Models**"""

#@title
print('Download pre-trained models...')
!mkdir ./checkpoints
!wget https://github.com/vinthony/video-retalking/releases/download/v0.0.1/30_net_gen.pth -O ./checkpoints/30_net_gen.pth
!wget https://github.com/vinthony/video-retalking/releases/download/v0.0.1/BFM.zip -O ./checkpoints/BFM.zip
!wget https://github.com/vinthony/video-retalking/releases/download/v0.0.1/DNet.pt -O ./checkpoints/DNet.pt
!wget https://github.com/vinthony/video-retalking/releases/download/v0.0.1/ENet.pth -O ./checkpoints/ENet.pth
!wget https://github.com/vinthony/video-retalking/releases/download/v0.0.1/expression.mat -O ./checkpoints/expression.mat
!wget https://github.com/vinthony/video-retalking/releases/download/v0.0.1/face3d_pretrain_epoch_20.pth -O ./checkpoints/face3d_pretrain_epoch_20.pth
!wget https://github.com/vinthony/video-retalking/releases/download/v0.0.1/GFPGANv1.3.pth -O ./checkpoints/GFPGANv1.3.pth
!wget https://github.com/vinthony/video-retalking/releases/download/v0.0.1/GPEN-BFR-512.pth -O ./checkpoints/GPEN-BFR-512.pth
!wget https://github.com/vinthony/video-retalking/releases/download/v0.0.1/LNet.pth -O ./checkpoints/LNet.pth
!wget https://github.com/vinthony/video-retalking/releases/download/v0.0.1/ParseNet-latest.pth -O ./checkpoints/ParseNet-latest.pth
!wget https://github.com/vinthony/video-retalking/releases/download/v0.0.1/RetinaFace-R50.pth -O ./checkpoints/RetinaFace-R50.pth
!wget https://github.com/vinthony/video-retalking/releases/download/v0.0.1/shape_predictor_68_face_landmarks.dat -O ./checkpoints/shape_predictor_68_face_landmarks.dat
!unzip -d ./checkpoints/BFM ./checkpoints/BFM.zip

"""**Inference**

`--face`: Input video.

`--audio`: Input audio. Both *.wav* and *.mp4* files are supported.

You can choose our provided data from `./examples` folder or upload from your local computer.





"""

#@title
import glob, os, sys
import ipywidgets as widgets
from IPython.display import HTML
from base64 import b64encode

print("Choose the Video name to edit: (saved in folder 'examples/face')")
vid_list = glob.glob1('examples/face/', '*.mp4')
vid_list.sort()
default_vid_name = widgets.Dropdown(options=vid_list, value='1.mp4')
display(default_vid_name)

print("Choose the Audio name to edit: (saved in folder 'examples/audio')")
audio_list = glob.glob1('examples/audio/', '*')
audio_list.sort()
default_audio_name = widgets.Dropdown(options=audio_list, value='1.wav')
display(default_audio_name)

"""Visualize the input video and audio:"""

#@title
input_video_name = './examples/face/{}'.format(default_vid_name.value)
input_video_mp4 = open('{}'.format(input_video_name),'rb').read()
input_video_data_url = "data:video/x-m4v;base64," + b64encode(input_video_mp4).decode()
print('Display input video: {}'.format(input_video_name), file=sys.stderr)
display(HTML("""
  <video width=400 controls>
        <source src="%s" type="video/mp4">
  </video>
  """ % input_video_data_url))

input_audio_name = './examples/audio/{}'.format(default_audio_name.value)
input_audio_mp4 = open('{}'.format(input_audio_name),'rb').read()
input_audio_data_url = "data:audio/wav;base64," + b64encode(input_audio_mp4).decode()
print('Display input audio: {}'.format(input_audio_name), file=sys.stderr)
display(HTML("""
  <audio width=400 controls>
        <source src="%s" type="audio/wav">
  </audio>
  """ % input_audio_data_url))

input_video_path = 'examples/face/{}'.format(default_vid_name.value)
input_audio_path = 'examples/audio/{}'.format(default_audio_name.value)

!python3 inference.py \
  --face {input_video_path} \
  --audio {input_audio_path} \
  --outfile results/output.mp4 \
  --img_size 512 \
  --pads 0 30 0 0 \
  --face_det_batch_size 8 \
  --LNet_batch_size 32 \
  --temporal_smooth_window 7 \
  --lip_temporal_smooth \
  --temporal_blend_weight 0.3

"""Visualize the output video:"""

#@title
# visualize code from makeittalk
from IPython.display import HTML
from base64 import b64encode
import os, sys, glob, cv2, subprocess, platform

def read_video(vid_name):
  video_stream = cv2.VideoCapture(vid_name)
  fps = video_stream.get(cv2.CAP_PROP_FPS)
  full_frames = []
  while True:
    still_reading, frame = video_stream.read()
    if not still_reading:
        video_stream.release()
        break
    full_frames.append(frame)
  return full_frames, fps

input_video_frames, fps = read_video(input_video_path)
output_video_frames, _ = read_video('./results/output.mp4')

frame_h, frame_w = input_video_frames[0].shape[:-1]
out_concat = cv2.VideoWriter('./temp/temp/result_concat.mp4', cv2.VideoWriter_fourcc(*'mp4v'), fps, (frame_w*2, frame_h))
for i in range(len(output_video_frames)):
  frame_input = input_video_frames[i % len(input_video_frames)]
  frame_output = output_video_frames[i]
  out_concat.write(cv2.hconcat([frame_input, frame_output]))
out_concat.release()

command = 'ffmpeg -loglevel error -y -i {} -i {} -strict -2 -q:v 1 {}'.format(input_audio_path, './temp/temp/result_concat.mp4', './results/output_concat_input.mp4')
subprocess.call(command, shell=platform.system() != 'Windows')


output_video_name = './results/output.mp4'
output_video_mp4 = open('{}'.format(output_video_name),'rb').read()
output_video_data_url = "data:video/mp4;base64," + b64encode(output_video_mp4).decode()
print('Display lip-syncing video: {}'.format(output_video_name), file=sys.stderr)
display(HTML("""
  <video height=400 controls>
        <source src="%s" type="video/mp4">
  </video>
  """ % output_video_data_url))

output_concat_video_name = './results/output_concat_input.mp4'
output_concat_video_mp4 = open('{}'.format(output_concat_video_name),'rb').read()
output_concat_video_data_url = "data:video/mp4;base64," + b64encode(output_concat_video_mp4).decode()
print('Display input video and lip-syncing video: {}'.format(output_concat_video_name), file=sys.stderr)
display(HTML("""
  <video height=400 controls>
        <source src="%s" type="video/mp4">
  </video>
  """ % output_concat_video_data_url))